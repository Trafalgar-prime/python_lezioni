# PyTorch: Output Layer e Loss per Tipo di Task

## Tabella di riferimento rapido

  -----------------------------------------------------------------------
  Tipo di Task            Descrizione             Esempio fisico
  ----------------------- ----------------------- -----------------------
  **Regressione üìà**      Prevedere un valore     Calcolare la
                          numerico continuo.      temperatura di una
                                                  stella.

  **Classificazione       Scegliere tra due       Decidere se un segnale
  Binaria ‚öñÔ∏è**            opzioni mutualmente     √® "Rumore" o "Evento".
                          esclusive.              

  **Classificazione       Scegliere **una sola**  Identificare un tipo di
  Multiclasse üè∑Ô∏è**        opzione tra 3 o pi√π     particella tra 5
                          categorie.              possibili.

  **Classificazione       Assegnare **pi√π         Un materiale pu√≤ essere
  Multilabel üìë**         etichette               "Magnetico" **e**
                          contemporaneamente**.   "Conduttore".
  -----------------------------------------------------------------------

------------------------------------------------------------------------

# 1) Regressione (valori continui)

Qui non classifichiamo: stimiamo una grandezza numerica.

-   **Ultimo layer**: `nn.Linear(n, 1)` (un solo neurone in uscita)
-   **Attivazione**: nessuna (identit√†) oppure **ReLU** se il valore
    deve essere solo positivo
-   **Loss**:
    -   `nn.MSELoss()` (Mean Squared Error)
    -   `nn.L1Loss()` (Mean Absolute Error)

------------------------------------------------------------------------

# 2) Classificazione binaria

-   **Ultimo layer**: `nn.Linear(n, 1)`
-   **Attivazione**: `torch.sigmoid()` (schiaccia l'output tra 0 e 1 ‚Üí
    interpretazione probabilistica)
-   **Loss**: `nn.BCELoss()` (Binary Cross Entropy)

------------------------------------------------------------------------

# 3) Classificazione multiclasse (single-label)

Il modello deve scegliere **una sola classe** tra **num_classi**
possibili.

Qui entra in gioco il concetto di: - **One-Hot Encoding** (target come
vettore con un solo "1" e il resto "0") oppure - **indici interi**
(target come intero 0..num_classi-1)

-   **Ultimo layer**: `nn.Linear(n, num_classi)`
-   **Attivazione**: `torch.softmax()` per ottenere una distribuzione di
    probabilit√† (somma = 1)
-   **Loss**: `nn.CrossEntropyLoss()`

### Nota tecnica (importantissima)

In PyTorch, `nn.CrossEntropyLoss()` **include gi√†** internamente
`LogSoftmax` + `NLLLoss`.\
Quindi l'ultimo layer del modello dovrebbe essere **lineare** (senza
softmax nel `forward`).

------------------------------------------------------------------------

# 4) Classificazione multilabel

Ogni etichetta √® trattata come un problema binario indipendente (S√¨/No
per ciascuna etichetta).

-   **Ultimo layer**: `nn.Linear(n, num_labels)`
-   **Attivazione**: `torch.sigmoid()` applicata a ogni neurone in
    uscita
-   **Loss**: `nn.BCEWithLogitsLoss()`

### Nota tecnica

`BCEWithLogitsLoss()` integra internamente la sigmoid in modo
numericamente stabile.\
In questo caso, l'ultimo layer del modello dovrebbe rimanere **lineare**
(logits).

------------------------------------------------------------------------

# Un dubbio sulla Multiclasse üß†

Nella classificazione multiclasse (es. distinguere tra 3 tipi di
galassie), il modello deve fornire **un'unica risposta**:\
cio√® **una sola classe** scelta tra tutte quelle possibili.

In PyTorch, la scelta dell'ultimo layer e della funzione di loss dipende
direttamente dal tipo di problema che stiamo affrontando. Di seguito una
guida strutturata per i quattro casi principali.

------------------------------------------------------------------------

## 1Ô∏è‚É£ Modello per Regressione (Valori Continui) üìà

Nel caso della regressione vogliamo prevedere un valore numerico
continuo (es. la massa di un pianeta, la temperatura di una stella, una
coordinata spaziale).

### üîπ Output Layer

nn.Linear(hidden_size, 1)

Un solo neurone in uscita perch√© dobbiamo prevedere un singolo valore
scalare.

### üîπ Attivazione Finale

Nessuna attivazione (identit√†) ‚Üí caso standard.

nn.ReLU() ‚Üí solo se sappiamo che il valore previsto deve essere
necessariamente positivo.

### üîπ Loss

nn.MSELoss()

oppure

nn.L1Loss()

MSELoss penalizza maggiormente errori grandi.

L1Loss (MAE) √® pi√π robusta agli outlier.

------------------------------------------------------------------------

## 2Ô∏è‚É£ Modello per Classificazione Binaria ‚öñÔ∏è

Qui dobbiamo scegliere tra due opzioni mutualmente esclusive (es.
"Segnale" vs "Rumore").

### üîπ Output Layer

nn.Linear(hidden_size, 1)

Un solo neurone che rappresenta la probabilit√† della classe positiva.

### üîπ Attivazione Finale

nn.Sigmoid()

La Sigmoid comprime l'output tra 0 e 1, permettendo un'interpretazione
probabilistica.

### üîπ Loss

nn.BCELoss()

‚ö† Nota importante: In alternativa (e pi√π stabile numericamente) si pu√≤
usare:

nn.BCEWithLogitsLoss()

In questo caso non bisogna applicare la Sigmoid nel modello, perch√© la
loss la integra internamente.

------------------------------------------------------------------------

## 3Ô∏è‚É£ Modello per Classificazione Multiclasse üè∑Ô∏è

Dobbiamo scegliere una sola categoria tra $N$ possibili (es. "Protone",
"Neutrone", "Elettrone").

### üîπ Output Layer

nn.Linear(hidden_size, num_classi)

Il numero di neuroni in uscita √® pari al numero di classi.

### üîπ Attivazione Finale

Nessuna attivazione nel modello (se si usa la loss standard di PyTorch).

### üîπ Loss

nn.CrossEntropyLoss()

‚ö† Nota Tecnica Fondamentale

nn.CrossEntropyLoss() combina internamente:

LogSoftmax

NLLLoss

Per questo motivo:

Il modello deve restituire logits grezzi (senza Softmax).

Non bisogna applicare Softmax nel forward().

La loss si occupa automaticamente della normalizzazione probabilistica.

------------------------------------------------------------------------

## 4Ô∏è‚É£ Modello per Classificazione Multilabel üìë

In questo scenario un campione pu√≤ appartenere a pi√π etichette
contemporaneamente (es. una stella pu√≤ contenere pi√π elementi chimici).

Ogni etichetta √® trattata come un problema binario indipendente.

### üîπ Output Layer

nn.Linear(hidden_size, num_labels)

Un neurone per ogni etichetta.

### üîπ Attivazione Finale

nn.Sigmoid()

Applicata separatamente a ogni neurone in uscita.

### üîπ Loss

Opzione consigliata:

nn.BCEWithLogitsLoss()

Oppure:

nn.BCELoss()

(se la Sigmoid √® gi√† stata applicata nel modello)

‚ö† Nota tecnica

BCEWithLogitsLoss() √® preferibile perch√©:

-   integra internamente la Sigmoid
-   √® pi√π stabile numericamente
-   evita problemi di saturazione

------------------------------------------------------------------------

## üìå Riassunto Concettuale

  Task          Output      Attivazione Finale   Loss
  ------------- ----------- -------------------- ---------------------
  Regressione   1 neurone   Nessuna              MSE / L1
  Binaria       1 neurone   Sigmoid              BCE / BCEWithLogits
  Multiclasse   N neuroni   Nessuna              CrossEntropy
  Multilabel    N neuroni   Sigmoid              BCEWithLogits